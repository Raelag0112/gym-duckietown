{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Render_in_gym.ipynb","provenance":[{"file_id":"1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t","timestamp":1580805585017},{"file_id":"1A75J2xFYjpJvNWXCSM1Xcty7K3WIGrud","timestamp":1542848369662}],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"GsSGgH2Y0apl","colab_type":"code","colab":{}},"source":["#          _____                _____                    _____                    _____                    _____                    _____          \n","#         /\\    \\              /\\    \\                  /\\    \\                  /\\    \\                  /\\    \\                  /\\    \\         \n","#        /::\\    \\            /::\\    \\                /::\\    \\                /::\\    \\                /::\\    \\                /::\\    \\        \n","#       /::::\\    \\           \\:::\\    \\              /::::\\    \\              /::::\\    \\              /::::\\    \\               \\:::\\    \\       \n","#      /::::::\\    \\           \\:::\\    \\            /::::::\\    \\            /::::::\\    \\            /::::::\\    \\               \\:::\\    \\      \n","#     /:::/\\:::\\    \\           \\:::\\    \\          /:::/\\:::\\    \\          /:::/\\:::\\    \\          /:::/\\:::\\    \\               \\:::\\    \\     \n","#    /:::/__\\:::\\    \\           \\:::\\    \\        /:::/__\\:::\\    \\        /:::/__\\:::\\    \\        /:::/__\\:::\\    \\               \\:::\\    \\    \n","#    \\:::\\   \\:::\\    \\          /::::\\    \\      /::::\\   \\:::\\    \\      /::::\\   \\:::\\    \\      /::::\\   \\:::\\    \\              /::::\\    \\   \n","#  ___\\:::\\   \\:::\\    \\        /::::::\\    \\    /::::::\\   \\:::\\    \\    /::::::\\   \\:::\\    \\    /::::::\\   \\:::\\    \\    ____    /::::::\\    \\  \n","# /\\   \\:::\\   \\:::\\    \\      /:::/\\:::\\    \\  /:::/\\:::\\   \\:::\\    \\  /:::/\\:::\\   \\:::\\____\\  /:::/\\:::\\   \\:::\\    \\  /\\   \\  /:::/\\:::\\    \\ \n","#/::\\   \\:::\\   \\:::\\____\\    /:::/  \\:::\\____\\/:::/  \\:::\\   \\:::\\____\\/:::/  \\:::\\   \\:::|    |/:::/  \\:::\\   \\:::\\____\\/::\\   \\/:::/  \\:::\\____\\\n","#\\:::\\   \\:::\\   \\::/    /   /:::/    \\::/    /\\::/    \\:::\\  /:::/    /\\::/   |::::\\  /:::|____|\\::/    \\:::\\  /:::/    /\\:::\\  /:::/    \\::/    /\n","# \\:::\\   \\:::\\   \\/____/   /:::/    / \\/____/  \\/____/ \\:::\\/:::/    /  \\/____|:::::\\/:::/    /  \\/____/ \\:::\\/:::/    /  \\:::\\/:::/    / \\/____/ \n","#  \\:::\\   \\:::\\    \\      /:::/    /                    \\::::::/    /         |:::::::::/    /            \\::::::/    /    \\::::::/    /          \n","#   \\:::\\   \\:::\\____\\    /:::/    /                      \\::::/    /          |::|\\::::/    /              \\::::/    /      \\::::/____/           \n","#    \\:::\\  /:::/    /    \\::/    /                       /:::/    /           |::| \\::/____/               /:::/    /        \\:::\\    \\           \n","#     \\:::\\/:::/    /      \\/____/                       /:::/    /            |::|  ~|                    /:::/    /          \\:::\\    \\          \n","#      \\::::::/    /                                    /:::/    /             |::|   |                   /:::/    /            \\:::\\    \\         \n","#       \\::::/    /                                    /:::/    /              \\::|   |                  /:::/    /              \\:::\\____\\        \n","#        \\::/    /                                     \\::/    /                \\:|   |                  \\::/    /                \\::/    /        \n","#         \\/____/                                       \\/____/                  \\|___|                   \\/____/                  \\/____/    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"odNaDE1zyrL2","colab_type":"text"},"source":["# install dependancies, takes around 45 seconds\n","\n","Rendering Dependancies\n","\n"]},{"cell_type":"code","metadata":{"id":"8-AxnvAVyzQQ","colab_type":"code","colab":{}},"source":["#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n","!pip install gym pyvirtualdisplay > /dev/null 2>&1\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n","\n","!apt-get update > /dev/null 2>&1\n","!apt-get install cmake > /dev/null 2>&1\n","!pip install --upgrade setuptools 2>&1\n","!pip install ez_setup > /dev/null 2>&1\n","!pip install gym[atari] > /dev/null 2>&1\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fvz2XHWYnkr6","colab_type":"code","colab":{}},"source":["\n","import gym\n","from gym import logger as gymlogger\n","from gym.wrappers import Monitor\n","gymlogger.set_level(40) #error only\n","import tensorflow as tf\n","import numpy as np\n","import random\n","import matplotlib\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import math\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","\n","from IPython import display as ipythondisplay\n","\n","from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1400, 900))\n","display.start()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8A-1LTSH88EE","colab_type":"text"},"source":["Pacman Dependancies"]},{"cell_type":"code","metadata":{"id":"TCelFzWY9MBI","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"APXSx7hg19TH","colab_type":"text"},"source":["# Imports and Helper functions\n"]},{"cell_type":"code","metadata":{"id":"G9UWeToN4r7D","colab_type":"code","colab":{}},"source":["\"\"\"\n","Utility functions to enable video recording of gym environment and displaying it\n","To enable video, just do \"env = wrap_env(env)\"\"\n","\"\"\"\n","\n","def show_video():\n","  mp4list = glob.glob('video/*.mp4')\n","  if len(mp4list) > 0:\n","    mp4 = mp4list[0]\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"Could not find video\")\n","    \n","\n","def wrap_env(env):\n","  env = Monitor(env, './video', force=True)\n","  return env"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W3BGbWOu179M","colab_type":"text"},"source":["# Pacman!"]},{"cell_type":"code","metadata":{"id":"dGEFMfDOzLen","colab_type":"code","colab":{}},"source":["env = wrap_env(gym.make('CartPole-v1'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7BmIlXhe9Q89","colab_type":"code","colab":{}},"source":["#check out the pacman action space!\n","print(env.action_space)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8nj5sjsk15IT","colab_type":"code","colab":{}},"source":["observation = env.reset()\n","\n","\n","for i in range(1000):\n","    env.reset()\n","    _,_,d,_ = env.step(np.random.randint(2))\n","    env.render()\n","    \n","    #your agent goes here\n","    action = env.action_space.sample() \n","         \n","    observation, reward, done, info = env.step(action) \n","   \n","        \n","    if done: \n","      print(i)\n","      break;\n","            \n","env.close()\n","show_video()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DAkKMVDdZsZN","colab_type":"code","colab":{}},"source":["class CartPoleSwingUp(gym.Wrapper):\n","    def __init__(self, env, **kwargs):\n","        super(CartPoleSwingUp, self).__init__(env, **kwargs)\n","        self.theta_dot_threshold = 4*np.pi\n","\n","    def reset(self):\n","        self.env.env.state = [0, 0, np.pi, 0] + super().reset()\n","        #self.state = [0,0,np.pi,0] + self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n","        self.env.env.steps_beyond_done = None\n","        return np.array(self.env.env.state)\n","\n","    def step(self, action):\n","        state, reward, done, _ = super().step(action)\n","        #self.state = state\n","        x, x_dot, theta, theta_dot = state\n","        \n","        done = x < -self.x_threshold \\\n","               or x > self.x_threshold \\\n","               or theta_dot < -self.theta_dot_threshold \\\n","               or theta_dot > self.theta_dot_threshold\n","        \n","        if done:\n","            # game over\n","            reward = -10.\n","            if self.steps_beyond_done is None:\n","                self.steps_beyond_done = 0\n","            else:\n","                logger.warn(\"You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\")\n","                self.steps_beyond_done += 1\n","        else:\n","            if -self.theta_threshold_radians < theta and theta < self.theta_threshold_radians:\n","                # pole upright\n","                reward = 1.\n","            else:\n","                # pole swinging\n","                reward = 0.\n","\n","        return np.array(self.state), reward, done, {}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rUTtqr93W9Ui","colab_type":"code","colab":{}},"source":["\n","env = wrap_env(CartPoleSwingUp(gym.make('CartPole-v1')))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c7Sft_8eaJm6","colab_type":"code","colab":{}},"source":["pong = gym.make('Pong-v4')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3rCPGI3taaGc","colab_type":"code","colab":{}},"source":["print(pong.observation_space)\n","print(pong.observation_space.shape)\n","print(np.min(pong.observation_space.low))\n","print(np.max(pong.observation_space.high))\n","print(pong.action_space)\n","#help(env.observation_space)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3-xD4LABaoT4","colab_type":"code","colab":{}},"source":["x = pong.reset()\n","plt.imshow(x)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xe7NdoHqaqUQ","colab_type":"code","colab":{}},"source":["from gym.wrappers import AtariPreprocessing\n","import cv2\n","\n","class PongWrapper(AtariPreprocessing):\n","    def __init__(self, env, **kwargs):\n","        super(PongWrapper, self).__init__(env, **kwargs)\n","    def step(self,action):\n","        return super(PongWrapper, self).step(4+action)\n","    def _get_obs(self):\n","        if self.frame_skip > 1:  # more efficient in-place pooling\n","            np.maximum(self.obs_buffer[0], self.obs_buffer[1], out=self.obs_buffer[0])\n","        obs = cv2.resize(self.obs_buffer[0], (84, 110), interpolation=cv2.INTER_AREA)[17:101,:]\n","\n","        if self.scale_obs:\n","            obs = np.asarray(obs, dtype=np.float32) / 255.0\n","        else:\n","            obs = np.asarray(obs, dtype=np.uint8)\n","        return obs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ig8ZYc31bhd5","colab_type":"code","colab":{}},"source":["env = wrap_env(PongWrapper(gym.make('PongNoFrameskip-v4'),\n","                   noop_max=0,\n","                   frame_skip=4,\n","                   terminal_on_life_loss=True,\n","                   grayscale_obs=True,\n","                   scale_obs=True))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wdNbN7azbj8d","colab_type":"code","colab":{}},"source":["# Trying a random agent in Pong\n","import time\n","\n","observation = env.reset()\n","env.render()\n","\n","for i in range(60):\n","    a = np.random.randint(2)\n","    x,r,_,_=env.step(a)\n","    env.render()\n","    #print('\\r', \"reward\", r, end=\"\")\n","    time.sleep(0.1)\n","            \n","env.close()\n","show_video()\n","\n","print(\"shape: \", x.shape, \", min = \", x.min(), \", max = \", x.max(), \", dtype = \", x.dtype, sep='')\n","plt.imshow(x, cmap='gray');"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JWoeSflIcJ_9","colab_type":"code","colab":{}},"source":["from collections import namedtuple\n","\n","\n","Transition = namedtuple('Transition',\n","                        ('state', 'action', 'next_state', 'reward', 'done'))\n","\n","\n","class ReplayMemory(object):\n","\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    def push(self, *args):\n","        \"\"\"Saves a transition.\"\"\"\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        self.memory[self.position] = Transition(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2SCGeYd4hNOb","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UVRSdd7Afcio","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class Net(nn.Module):\n","\n","    def __init__(self):\n","      \n","        super(Net, self).__init__()\n","        \n","        self.fc1 = nn.Linear(4, 16)  # 6*6 from image dimension\n","        self.fc2 = nn.Linear(16,16)\n","        self.fc3 = nn.Linear(16, 2)\n","\n","    def forward(self, x):\n","        \n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","\n","\n","net = Net()\n","print(net)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0kizwTIklbmP","colab_type":"code","colab":{}},"source":["env = wrap_env(gym.make('CartPole-v1'))\n","\n","#check out the pacman action space!\n","print(env.action_space)\n","print(env.observation_space)\n","\n","from torch import optim\n","\n","BATCH_SIZE = 512\n","GAMMA = 0.9\n","EPS_START = 0.9\n","EPS_END = 0.05\n","EPS_DECAY = 200\n","TARGET_UPDATE = 10\n","epsilon = 0.5\n","\n","\n","# Get number of actions from gym action space\n","n_actions = env.action_space.n\n","print(n_actions)\n","\n","policy_net = Net()\n","target_net = Net()\n","target_net.load_state_dict(policy_net.state_dict())\n","target_net.eval()\n","\n","optimizer = optim.RMSprop(policy_net.parameters())\n","memory = ReplayMemory(10000)\n","\n","\n","steps_done = 0\n","\n","\n","def select_action(state,NET):\n","    global steps_done\n","    sample = random.random()\n","    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n","        math.exp(-1. * steps_done / EPS_DECAY)\n","    steps_done += 1\n","    if sample > eps_threshold:\n","        with torch.no_grad():\n","            # t.max(1) will return largest column value of each row.\n","            # second column on max result is index of where max element was\n","            # found, so we pick action with the larger expected reward.\n","            return NET(state.float()).max(1)[1].view(1, 1)\n","    else:\n","        return torch.tensor([[random.randrange(n_actions)]], dtype=torch.long)\n","\n","\n","episode_durations = []"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VhHAIfG1C5Y9","colab_type":"code","colab":{}},"source":["policy_net"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LautmTMqlc_P","colab_type":"code","colab":{}},"source":["def optimize_model():\n","    if len(memory) < BATCH_SIZE:\n","        return\n","    transitions = memory.sample(BATCH_SIZE)\n","\n","    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n","    # detailed explanation). This converts batch-array of Transitions\n","    # to Transition of batch-arrays.\n","\n","    batch = Transition(*zip(*transitions))\n","\n","    # Compute a mask of non-final states and concatenate the batch elements\n","    # (a final state would've been the one after which simulation ended)\n","    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n","                                          batch.next_state)), dtype=torch.bool)\n","    non_final_next_states = torch.cat([torch.tensor(s) for s in batch.next_state\n","                                                if s is not None]).view(BATCH_SIZE,4)\n","    \n","    \n","    state_batch = torch.cat(batch.state).view(BATCH_SIZE,4)\n","    # action_batch = torch.cat(batch.action)\n","    reward_batch = torch.cat(batch.reward)\n","\n","    action_batch = torch.tensor([torch.tensor(s) for s in batch.action]).view(BATCH_SIZE,1)\n","    # reward_batch = torch.cat([torch.tensor(s) for s in batch.reward])\n","\n","    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n","    # columns of actions taken. These are the actions which would've been taken\n","    # for each batch state according to policy_net\n","\n","\n","\n","    policy_out = policy_net(state_batch.float())\n","\n","    state_action_values = torch.zeros(BATCH_SIZE)\n","    \n","    for i in range(BATCH_SIZE):\n","      \n","      if action_batch[i] == 0:\n","        state_action_values[i] = policy_out[i][0]\n","      else :\n","        state_action_values[i] = policy_out[i][1]\n","\n","\n","    \n","    \n","    # Compute V(s_{t+1}) for all next states.\n","    # Expected values of actions for non_final_next_states are computed based\n","    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n","    # This is merged based on the mask, such that we'll have either the expected\n","    # state value or 0 in case the state was final.\n","    next_state_values = torch.zeros(BATCH_SIZE)\n","    next_state_values[non_final_mask] = target_net(non_final_next_states.float()).max(1)[0].detach()\n","    # Compute the expected Q values\n","    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n","    # Compute Huber loss\n","    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n","    \n","    # Optimize the model\n","    optimizer.zero_grad()\n","    loss.backward()\n","    for param in policy_net.parameters():\n","        param.grad.data.clamp_(-1, 1)\n","    optimizer.step()\n","    return loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sfLdBNpY1tXt","colab_type":"code","colab":{}},"source":["env = wrap_env(CartPoleSwingUp(gym.make('CartPole-v1')))\n","\n","#check out the pacman action space!\n","print(env.action_space)\n","print(env.observation_space)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Armlw8SeQIHS","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","state_dim = env.observation_space.shape[0]\n","n_action = env.action_space.n \n","nb_neurons=24\n","\n","DQN = torch.nn.Sequential(nn.Linear(state_dim, nb_neurons),\n","                          nn.ReLU(),\n","                          nn.Linear(nb_neurons, nb_neurons),\n","                          nn.ReLU(), \n","                          nn.Linear(nb_neurons, n_action)).to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9bIaBADSQIMJ","colab_type":"code","colab":{}},"source":["# It will actually be useful to have separate torch.Tensor for the each element type in the sampled minibatch.\n","# That is one Tensor for a minibatch of states, another for actions, etc.\n","# Let's redefine the sample function of our replay buffer class to that end.\n","import random\n","\n","class ReplayBuffer:\n","    def __init__(self, capacity):\n","        self.capacity = capacity # capacity of the buffer\n","        self.data = []\n","        self.index = 0 # index of the next cell to be filled\n","\n","    def append(self, s, a, r, s_, d):\n","        if len(self.data) < self.capacity:\n","            self.data.append(None)\n","        self.data[self.index] = (s, a, r, s_, d)\n","        self.index = (self.index + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        batch = random.sample(self.data, batch_size)\n","        return list(map(lambda x:torch.Tensor(x).to(device), list(zip(*batch))))\n","\n","    def __len__(self):\n","        return len(self.data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fQlajtpcQIE6","colab_type":"code","colab":{}},"source":["# Let's define a utility function that gives us the greedy action from a DQN\n","import torch\n","\n","def greedy_action(network, state):\n","    with torch.no_grad():\n","        Q = network(torch.Tensor(state).unsqueeze(0).to(device))\n","        return torch.argmax(Q).detach().cpu().item()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pPwIQbU-QIDI","colab_type":"code","colab":{}},"source":["# The Deep Q-learning algorithm's class\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","\n","class DQN_agent:\n","    def __init__(self, config, model):\n","        self.gamma = config['gamma']\n","        self.batch_size = config['batch_size']\n","        self.nb_actions = config['nb_actions']\n","        self.memory = ReplayBuffer(config['buffer_size'])\n","        self.epsilon_max = config['epsilon_max']\n","        self.epsilon_min = config['epsilon_min']\n","        self.epsilon_stop = config['epsilon_stop']\n","        self.epsilon_delay = config['epsilon_delay_decay']\n","        self.epsilon_step = (self.epsilon_max-self.epsilon_min)/self.epsilon_stop\n","        self.total_steps = 0\n","        self.model = model \n","        self.criterion = torch.nn.MSELoss()\n","        self.optimizer = torch.optim.RMSprop(self.model.parameters(), lr=config['learning_rate'])\n","    \n","    def gradient_step(self):\n","        if len(self.memory) > self.batch_size:\n","            X, A, R, Y, D = self.memory.sample(self.batch_size)\n","            QX = self.model(X)\n","            QY = self.model(Y)\n","            QYmax = torch.max(QY, axis=1)[0]\n","            update = QX\n","            A = A.to(torch.long)\n","            update[torch.arange(self.batch_size), A] = torch.addcmul(R, self.gamma, 1-D, QYmax)\n","\n","            loss = self.criterion(self.model(X), update)\n","            self.optimizer.zero_grad()\n","            loss.backward()\n","            self.optimizer.step() \n","    \n","    def train(self, env, max_episode):\n","        episode_return = []\n","        episode = 0\n","        episode_cum_reward = 0\n","        state = env.reset()\n","        epsilon = self.epsilon_max\n","        step = 0\n","\n","        while episode < max_episode:\n","            # update epsilon\n","            if episode == max_episode-1:\n","                \n","                action = greedy_action(self.model, state)\n","\n","                 \n","                if step > self.epsilon_delay:\n","                    epsilon = max(self.epsilon_min, epsilon-self.epsilon_step)\n","\n","                action = greedy_action(self.model, state)\n","\n","                # step\n","                next_state, reward, done, _ = env.step(action)\n","                episode_cum_reward += reward\n","\n","                # next transition\n","                step += 1\n","                if done:\n","                    episode += 1\n","                    print(\"Episode \", '{:3d}'.format(episode), \n","                          \", epsilon \", '{:6.2f}'.format(epsilon), \n","                          \", batch size \", '{:5d}'.format(len(self.memory)), \n","                          \", episode return \", '{:4.1f}'.format(episode_cum_reward),\n","                          sep='')\n","                    state = env.reset()\n","                    episode_return.append(episode_cum_reward)\n","                    episode_cum_reward = 0\n","                else:\n","                    state = next_state\n","                    env.render()\n","\n","\n","\n","\n","\n","            else : \n","                if step > self.epsilon_delay:\n","                    epsilon = max(self.epsilon_min, epsilon-self.epsilon_step)\n","\n","                # select epsilon-greedy action\n","                if np.random.rand() < epsilon:\n","                    action = np.random.randint(self.nb_actions)\n","                else:\n","                    action = greedy_action(self.model, state)\n","\n","                # step\n","                next_state, reward, done, _ = env.step(action)\n","                self.memory.append(state, action, reward, next_state, done)\n","                episode_cum_reward += reward\n","\n","                # train\n","                self.gradient_step()\n","\n","                # next transition\n","                step += 1\n","                if done:\n","                    episode += 1\n","                    print(\"Episode \", '{:3d}'.format(episode), \n","                          \", epsilon \", '{:6.2f}'.format(epsilon), \n","                          \", batch size \", '{:5d}'.format(len(self.memory)), \n","                          \", episode return \", '{:4.1f}'.format(episode_cum_reward),\n","                          sep='')\n","                    state = env.reset()\n","                    episode_return.append(episode_cum_reward)\n","                    episode_cum_reward = 0\n","                else:\n","                    state = next_state\n","            \n","\n","        show_video()\n","\n","        return episode_return"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UCyBkJLiQH_w","colab_type":"code","colab":{}},"source":["from gym import logger\n","env.close()\n","# env = wrap_env(CartPoleSwingUp(gym.make('CartPole-v1')))\n","env = wrap_env(gym.make('CartPole-v1'))\n","\n","\n","config = {'observation_space': env.observation_space.shape[0],\n","          'nb_actions': env.action_space.n,\n","          'learning_rate': 0.001,\n","          'gamma': 0.95,\n","          'buffer_size': 1000000,\n","          'epsilon_min': 0.01,\n","          'epsilon_max': 1.,\n","          'epsilon_stop': 1000,\n","          'epsilon_delay_decay': 20,\n","          'batch_size': 20}\n","\n","agent = DQN_agent(config, DQN)\n","scores = agent.train(env, 200)\n","plt.plot(scores);\n","\n","env.close()\n","show_video()"],"execution_count":0,"outputs":[]}]}