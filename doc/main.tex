\documentclass[12pt]{article}
\usepackage{graphicx} % This lets you include figures
\usepackage{hyperref} % This lets you make links to web locations
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{amsmath}
\graphicspath{ {./images/} }

\usepackage[rightcaption]{sidecap}
\usepackage{subcaption}
\usepackage{wrapfig}

\usepackage{float}

\usepackage{imakeidx}

\usepackage{listings}

\usepackage{pgf-umlsd}

\usepackage{amssymb}

\makeindex


\title{SIM2REAL}
\author{Olivier Sutter \and Mathieu Verm \and Michael Romagne \and Alexandre Martin \and Pablo Mirales \and Vincent Coyette}

\date{\today}

\begin{document}
\maketitle{}

\tableofcontents

\clearpage
\newpage

\section{Introduction}
Reinforcement Rearning is an area of machine learning. The goal is for an agent to learn an optimal behaviour and adapt its comportement, based on experience, in a given environment . Reinforcement Learning has been very successful in the last few years, notably for its great performance in games (e.g. AlphaGo\footnote{Silver, D., Schrittwieser, J., Simonyan, K. et al. Mastering the game of Go without human knowledge. Nature 550, 354â€“359 (2017)}). However, it suffers from a lack of industrial applications.

One of the reason for this is that an agent needs a lot of experience in order to learn a reasonable behaviour. Those simulations must be run in a simulator for safety, speed and financial reasons. Moreover, the transfer of behaviour from the simulated agent to the real world (for example an autonomous vehicle) is far from being trivial. Even for high quality simulators, there is a shift in both states and transitions spaces between the simulator and the real world.

The purpose of this project is to study different methods to robustly train an autonomous car in a simulator, so that the real-world agent behaves as suited.


DuckieTown\footnote{See https://www.duckietown.org/} environment will be used during all the project. It provides a simulator (Gym-Duckietown), a real-world agent and a real-world environment. 


\section{Environment}
An environment in which a Reinforcement Learning agent is trained is represented by a Markov Decision Process (MDP). A MDP is basically a 4-tuple $(S, A, P_a, R_a)$. Let's detail each of these components for duckietown. 

\subsection{State}
In a MDP, the state is defined by the state of the environment as it is seen by the agent. Here, the robot contains only one sensor, the camera. The camera has a resolution of 160x120, and thus the state space is $S = [0, 255]^{160x120x3}$ (3 is for the 3 RGB colors).

\subsection{Actions}
The actions of the environment can take are composed by a forward velocity and a steering angle. By default, the action space is continuous, $A = [-1, 1]^2$:
\begin{itemize}
    \item The first number corresponds to the forward velocity. 1 is for going full speed forward, -1 is for full speed backward.
    \item The second number corresponds to the steering angle. 1 is for the steering wheel being fully pushed to the left.
\end{itemize}

The action space can be changed to be discrete. In this case, the possible actions would be moving forward, turning left or turning right. This maps to the continous space as follows:

\begin{itemize}
    \item Forward -> (0.7, 0.0)
    \item Right -> (0.6, 1.0)
    \item Left -> (0.6, -1.0)
\end{itemize}

\subsection{Transitions}
The transition of a MDP is the distribution $P(s'| s, a)$, i.e. the probability of ending up in the state $s'$ if the agent is in the state s and take the action a. The transitions would typically be different inside the simulator and in real life.

Inside the simulator, the transitions are considered as perfect. For an agent being at a given position and taking a certain action  ${speed, angle}$, the simulator updates the position of the agent accordingly. The state $s'$ is then the new camera signal. The state is not fully deterministic, as some objects (such as duckie pedestrians for example) may have a stochastic behaviour and appear on the camera, but the updated position of the agent is a deterministic function of the current
position and the current action.

In the real world however, the transitions between the positions are not perfect. Given a position and an action, the next position is not deterministic, as it depends on additional variables unknown to the agent. It can slightly vary because of mechanical links not being perfect, or because of an unusual road surface. This shift in the transition distribution is one of the problem we will have to address during the simulation to reality transfer.

\subsection{Reward}
The choice of the reward function is primordial to properly train the agent. The reward may depend on :

\begin{itemize}
    \item The position of the agent. The position may be used to compute the distance from a target position, and the distance from the middle of the lane.
    \item The speed of the agent. The faster the better.
    \item Collisions. The agent must get a negative reward in case of collision, and possibly if it gets too  close to another object.
\end{itemize}

The default reward in duckietown is as follows:

\begin{align}
R(t) = 40 * C_{p} - 10*dist + alignment*speed \\
C_p = \sum_{Obj} ((pos_{agent} - pos_{object}) - SR_{agent} - SR_{object}) \\
\end{align}
\begin{itemize}
    \item $C_p$ is the collision penalty for being dangerously close to other objects. It is a proxy for area overlap. Notably, one can collide with several objects at once (with additive effects) and one can collide with respect to the reward function (which uses the safety radius) without the episode restarting (which depends on the collision radius). Note that collision penalty is smaller than 0 whenever there is a collision, therefore this is actually a penalty despite the positive sign.
    \item dist is the distance between the agent's center and the closest point on the spline defining the lane's center
    \item alignment is the dot product between direction and the normalized tangent of the road.
    \item speed is the agent's speed
\end{itemize}

This reward function has presented several issues:

\begin{enumerate}
    \item The penalties (collision and distance) are so strong that the agent usually much prefers to go around in circles at max speed at the center of the lane than to actually follow it.
\end{enumerate}


The design of the reward function will condition the performance of our agent in the simulator. This reward will be used to train a policy, i.e. a mapping from a state $s$ to an action $a$ to take. This policy will need to be adapted before transfer to the real world. However, the reward does not need to be adapted. 

\section{Gym-DuckieTown Simulator}

From the README.md in the github repository\footnote{https://github.com/duckietown/gym-duckietown}, here is an introduction to Gym-Duckietown. 

 \begin{quotation}
Gym-Duckietown is a simulator for the Duckietown Universe, written in pure Python/OpenGL (Pyglet). It puts the agent, a Duckiebot, inside of an instance of a Duckietown: a loop of roads with turns, intersections, obstacles, Duckie pedestrians, and other Duckiebots. It can be a pretty hectic place!
 \end{quotation}

 This project has a dedicated github repository\footnote{https://github.com/vcoyette/gym-duckietown}, which is a fork from the Gym-Duckietown original repo.

\subsection{Installation}
The installation is pretty straight-forward from the repository. Use the following commands :

\begin{lstlisting}[language=bash]
    git clone https://github.com/vcoyette/gym-duckietown
    cd gym-duckietown
    conda env create -f environment.yaml
\end{lstlisting}

The installation has been tested on Windows, Linux and MacOS. Some problems may be encountered for the installation of certain packages. They can be resolved with package-specific installation instructions. 
For example, the installation of pyglet may raise an issue. It can be resolved by installing it from the pyglet github repository. 

To use the simulator, the environment must be activated :
 
\begin{lstlisting}[language=bash]
    conda activate gym-duckietown
\end{lstlisting}

And the root folder of the project must be added to the PYTHONPATH environment variable.
On linux :
\begin{lstlisting}[language=bash]
    export PYTHONPATH="${PYTHONPATH}:`pwd`"
\end{lstlisting}

On Windows, environment variable can be accessed in the parameters,  advanced parameters. You can then append the path of your project folder to the PYTHONPATH variable if it exists, or create it otherwise.


\subsection{Manual Control}
A UI application can be launched to manually control the robot. Actions can be sent from the keyboard, and images from the DuckieBot camera are displayed. Here is a simple command to launch the application :

\begin{lstlisting}{laguage=bash}
$ ./manual_control.py --env-name Duckietown-udem1-v0
\end{lstlisting}

\subsubsection{Parameters}
Here is a list of parameters which can be used. 

\begin{itemize}
	\item env-name: the name of the environment to execute (TODO anchor to env description)
	\item map-name: the name of the map to be used
	\item distortion: boolean, add distorsion
	\item draw-curve: boolean, draw the lane-following curve
	\item draw-bbox: boolean, draw the collision bounding box
	\item domain-rand: boolean, use domain randomization
	\item frame-skip: number of frames to skip (default 1)
	\item seed: seed
\end{itemize}

\subsubsection{Keyboard}
Here is a list of keys which can be used during the simulation.

\begin{itemize}
	\item Escape : exit simulation
	\item Backspace : restart simulation
	\item Directional keys: go forward, backward or turn
	\item Shift : boost speed
\end{itemize}

\subsubsection{Logs}
The github repository contains a branch "experiment". This branch is to be used to do any experiment on the simulator. The main difference from a classical "develop" branch is that it does not aim at being merged into the master branch. 

In this branch, we designed a logger for the manual control application. At each time step, this logger will log the current position of the agent, the current speed, the current distance from the lane, the action took. The goal is to manually control the agent so that it behaves as we would expect. We would then be able to check the logs, and use them to design a reward function.

To enable this option, pass the --output option to manual\_control.py. You can optionally specify another option `--filename example.csv` to specify the name of the output file, which would be in data/example.csv. If no file-name is specified, the logs will be stored in data/manual\_controli.csv, where i is the first number for which this path is free.

When the episode is done, the manual control must be exited by pressing the S key to save the output.

The learning is performed using the train\_reinforcement.py script. In order to execute it, change into the directory learning and run : 

\begin{lstlisting}{laguage=bash}
    python -m reinforcement.pytorch.train_reinforcement
\end{lstlisting}

If an error is returned stating that a module doesn't exist, check if PYTHONPATH variable contains the repository base folder.

\begin{lstlisting}{laguage=bash}
    echo $PYTHONPATH
\end{lstlisting}

If it doesn't, get back to your to the root directory of the project and run: 

\begin{lstlisting}{laguage=bash}
    export PYTHONPATH="${PYTHONPATH}:`pwd`"
\end{lstlisting}

The default algorithm implemented is DDPG. It can be tweaked by passing some hyperparameters when executing train\_reinforcement. A list of the parameters can be accessed by running :

\begin{lstlisting}{language=bash}
    python -m reinforcement.pytorch.train_reinforcement --h
\end{lstlisting}

\subsection{Create Maps}
You can very easily create a new \textbf{Duckietown} environment with a text editor. A Duckietown's map is a \textit{.yaml} file, so you have to save your new map in the folder \textbf{maps} as "\textit{my\_new\_map.yaml}" (the path should looks like this one : \textit{./gym-duckietown-master/gym\_duckietown/maps}).
\newline

If you want to see your map, you can use the following line in the terminal : \newline

\begin{lstlisting}{language=bash}
./manual_control.py --env-name Duckietown-udem1-v0 
--map-name my\_new\_map}
\end{lstlisting}

\noindent Your robot will be manually controlled in your map.\newline


\noindent A grassroots level of your \textit{.yaml} should looks like this :\newline
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

\noindent tiles:\newline

\noindent- [floor, floor, floor        , grass        , grass        , grass, floor, floor]\newline
\noindent- [floor, floor       , grass        , grass        , straight/S   , grass, floor, floor]\newline
\noindent- [floor, grass       , grass        , curve\_left/W , curve\_right/S, grass, floor, floor]\newline
\noindent- [grass, grass       , curve\_left/W , curve\_right/S, grass        , grass, floor, floor]\newline
\noindent- [grass, curve\_left/W, curve\_right/S, grass        , grass        , floor, floor, floor]\newline
\noindent- [grass, curve\_left/S, curve\_right/E, grass        , grass        , floor, floor, floor]\newline
\noindent- [grass, grass       , curve\_left/S , curve\_right/E, grass        , floor, floor, floor]\newline
\noindent- [floor, grass       , curve\_left/W , curve\_right/S, grass        , floor, floor, floor]\newline
\noindent- [floor, grass       , straight/S   , grass        , grass        , floor, floor, floor]\newline
\noindent- [floor, grass       , grass        , grass        , floor        , floor, floor, floor]\newline
\noindent- [floor, floor       , floor        , floor        , floor        , floor, floor, floor]\newline

\noindent objects:\newline

\noindent- kind: house\newline
  pos: [4.5, 9.1]\newline
  rotate: 90\newline
  height: 0.5\newline

\noindent- kind: tree\newline
  pos: [1, 1]\newline
  rotate: 0\newline
  height: 0.5\newline

\noindent- kind: tree\newline
  pos: [2, 8.5]\newline
  rotate: 90\newline
  height: 0.3\newline

\noindent tile\_size: 0.585\newline
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\newline

For each line, the number of tiles has to remain the same. The \textbf{tile\_size} and the \textbf{height} of every object can change, but insofar as your goal is to transpose the simulation to the real world, you must not change their values. If so, you should have \textbf{tile\_size}$=0.585$ (for the conventional heights of the objects will be given below).\newline

You can find your way in the map knowing that going upward is going North, and knowing that when you drive on a road tile, the name of the road tile tells you which direction you were facing when you arrived on it, and by which direction you will leave the tile. Let's give an example : the tile "\textit{curve\_left/W}" means that you arrived on it while you were moving West, and the fact that you'll turn left on it says you are going to leave the tile by the South (it's of course reversible, you can arrive from South facing North, and leave to the East).\newline
You can also know your position $(x,\ y)\in\mathbb{R}^2$ on the map (this is the way you put objects on it). The point $[0,\ 0]$ matches the upper left corner of the upper left tile of the map (so the coordinates start at the very North/West). The coordinates keep going higher as you move toward South/East, increasing of 1 each time you go through one tile.\newline

Here is the list of the tiles you can use to build your map:

\noindent- straight\newline
\noindent- curve\_left\newline
\noindent- curve\_right\newline
\noindent- 3way\_left (3-way intersection)\newline
\noindent- 3way\_right\newline
\noindent- 4way (4-way intersection)\newline
\noindent- asphalt\newline
\noindent- grass\newline
\noindent- floor (office floor)\newline

You can (and should) orientate the roads (the six first tiles on the list) by adding "/N", "/E", "/S", "/W" (this will be oriented according to the rule given above).\newline

The objects can be added as shown in the following example (changing the name of the object). Here is a list of them :\newline

\noindent- barrier (height : 0.08)\newline
\noindent- cone (height : 0.08)\newline
\noindent- duckie (height : as you wish between 0.06 and 0.08)\newline
\noindent- duckiebot (height : 0.12)\newline
\noindent- tree (height : as you wish between 0.1 and 0.9)\newline
\noindent- house (height : 0.5)\newline
\noindent- truck (height : 0.25)\newline
\noindent- bus (height : 0.18)\newline
\noindent- building (height : 0.6)\newline
\noindent- sign\_stop, sign\_T\_intersect, sign\_yield, etc... (height : $0.18$ for the signs, $0.4$ for a traffic light)\newline

There are many other signs, you can check the whole list here :\newline
https://github.com/duckietown/gym-duckietown/blob/master/gym\_duckietown/meshes\newline

It is possible to add the attributes : \newline

\noindent- optional: True or False (makes the object optional)\newline
\noindent- static: True or False (for the Duckiebot for example if you want to see them move)\newline

To go any further about map creation, check the Github of Duckietown on this link : https://github.com/vcoyette/gym-duckietown/tree/documentation#design\newline

\subsection{Reward Wrapper}
The reward function is computed by the compute\_reward function defined in the class Simulator (in the file gym\_duckietown/simulator.py). The best way to modify the reward function is to edit the function reward of the DtRewardWrapper class in the file learning/utils/wrappers.py.

This class is a subclass of the gym RewardWrapper class. This class implements the Wrapper/Adapter design pattern. It overrides the step function of the simulator. It is hard to explain, but you can have a look at the following sequence diagram to get an idea of how it works.

The idea is that the reward wrapper replaces the environment in the DDPG implementation. During the training, the DDPG calls the step function of the wrapper. This wrapper instance has an attribute which is the environment. The step function in the wrapper calls the function step of the environment, and captures the returned values (observation, reward, done and info). Then, the wrapper runs its own reward function with the reward returned by the previous call as parameter. The wrapper step function returns the values of observation, done and info which were returned by the environment, and returns its custom reward to replace the reward of the environment.

The wrapper containing an environment attribute, it has access to the same information as the simulator to compute its custom reward. For example, it can use the current position through env.cur\_pos, the angle through env.angle, etc.


\begin{sequencediagram}
    \newthread{t}{DDPG}
    \newinst[1]{w}{DtRewardWrapper}
    \newinst[2]{s}{Simulator}
    \begin{call}{t}{step()}{w}{\shortstack{
        return obs, \\ reward(reward) \\
        done, \\ info }}
    

        \begin{call}{w}{step()}{s}{\shortstack{
            return obs, \\ reward, \\
            done, \\ info }}
            \postlevel
            \postlevel
            \postlevel
        \end{call}
    \postlevel
    \postlevel
    \postlevel
    \end{call}
\end{sequencediagram}

TODO : add legend to diagram



\subsection{Domain Randomisation}
\subsubsection{The randomisation API}
The folder at \url{gym-duckietown/gym_duckietown/randomization} contains the domain randomization API. This API contains all of the pre-packaged methods for randomization within gym-duckietown. This folder also contains a readme file detailing the API.

The domain randomization is driven by the Randomizer class, which takes as input a configuration file and outputs (upon call to randomize) a set of settings used by the Simulator class (the core class of gym\-duckietown managing the environment). To activate any domain randomization at all, the simulator class must have domain\_rand \= true passed as a parameter to its constructor

If a randomizable variable is not found in the configuration file, it will be randomized according to the default values found in \url{gym-duckietown/gym_duckietown/randomization/config/default.json}. 3 types of distribution are supported, int, uniform and normal. 4 variables are randomizable by default:
\begin{enumerate}
    \item horz\_mode: The task is made harder by making the horizon more similar to the road. It can take integer values from 0 to 3 where:
    \begin{enumerate}
        \item 0: Sets the skybox to a blue sky, the default
        \item 1: Sets the skybox to a gray wall, intended to be similar to room testing conditions
        \item 2: Sets the skybox to a dark gray box
        \item 3: Sets the skybox to a light gray box
    \end{enumerate}
    \item light\_pos: Makes the simulator more or less iluminated, by changing the position of the single light source.
    \item camera\_noise: Adds noise to the camera position for data augmentation purposes. This noise is applied at each render, giving the screen input a "twitchy" behaviour.
    \item frame\_skip: No info provided. Code inspection shows this is the number of frames to skip per action. Higher frameskip reduces the ability of the agent to change its action. Beyond randomization, frame skip is enabled by default (1 frame skipped) and its value can be passed as an argument during simulator construction.
\end{enumerate}

The API is sorely lacking in the amount of variables that can be randomized. It provides some flexibility in randomization of the input, but it provides no randomization besides frame\_skip of transitions and/or actions.
\subsubsection{Non API Randomisation}
Code inspection reveals that more randomisation occurs, contingent on domain-rand being activated. 
\begin{enumerate}
    \item Within Simulator.py: All calls to \_perturb(val,scale) distort the value passed as argument if and only if domain rand is true, otherwise they return it undisturbed. The distortion is a multiplicative \% error drawn uniformly between $1 - scale$ and $1 + scale$, with a default of $scale = 0.1$ (e.g. by default values are randomised between 90 and 110\%). The function is called on:
    \begin{itemize}
        \item horizon\_color, beyond the randomization introduced by horz\_mode. 10\% for blue\_sky and wall\_color sky (modes 1 and 2), 40\% for modes 3 (dark gray) and 4 (clear gray)
        \item The parameters for glLight functions. I am currently unaware of the effect of this function. TODO: FIND OUT
        \item ground\_color, 30\%
        \item wheel\_dist, 10\%
        \item cam\_height, 8\%
        \item cam\_angle, 20\%
        \item cam\_fov\_y, 20\%, camera field of view
        \item side length of the ground/noise triangles generated as distractors (which themselves are generated randomly in the first place? Seems redundant to do it twice), 10\%
        \item tile color, 20\% for each tile
        \item object color, 20\% for each color
        \item CAMERA\_FORWARD\_DIST of gl.glTranslatef on line 1434, 10\%
        \item The actual tile texture loaded is randomised with randint amongst all possible candidates
        \item Some optional objects are invisible, 33\% chance
    \end{itemize}
    \item Within objects.py
    \begin{itemize}
        \item DuckiebotObj (Cars)
        \begin{itemize}
            \item follow\_dist is randomised from 0.3 to a uniform between 0.3 and 0.4
            \item velocity is randomised from 0.1 to a uniform between 0.05 and 0.15
        \end{itemize}
        \item DuckieObj (Pedestrians)
        \begin{itemize}
            \item pedestrian\_wait\_time randomised from 8 to a randint from 3 to 20, takes on new random value on same range when finish crossing street
            \item vel randomised from 0.02 to a normal with avg 0.02 and stdev 0.005, takes on new random value on same range when finish crossing street
        \end{itemize}
        \item TrafficLightObj
        \begin{itemize}
            \item freq is randomised from 5 with randint(4,7)
            \item The lights start randomly from off as either On or Off, 50\% chance each
        \end{itemize}
    \end{itemize}
\end{enumerate}


\subsubsection{Randomizing inputs}


Randomizing inputs can be accomplished via the API. Other possible sources of randomization are:
\begin{itemize}
    \item Changing hue of key elements: stop signs, roadâ€¦ Might be hard since theyâ€™re all texture based. A possible approach: Generating variations on existing textures: Automatically generate X diff textures with some other software from existing textures passed through some filters, then use the in-built texture randomiser
    \item Adding noise to the camera acquisition (not position)? This is similar to randomising the color of the sky and objects themselves so it might not be interesting. However camera noise provides variations during a single episode or each timestep which may imitate real camera noise (or not?)
    \item Approaching photorrealism in some way can be helpful in improving the performance (Johnson-Robertson et al.) May prove unfeasible given the simulator and available time and resources
    \item Addding additional light sources?
    \item Gaussian noise foreground of images and edge blurring through gaussian noise at edges on input images is also helpful (Hinterstoisser et al.) The technique can be extended to other blending techniques (Dwibedi et al.)
\end{itemize}
\subsubsection{Randomizing transitions}
\subsubsection{Randomizing actions}



\end{document}

