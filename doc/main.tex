%\documentclass{article}
%\usepackage[utf8]{inputenc}

\documentclass[12pt]{article}
\usepackage{graphicx} % This lets you include figures
\usepackage{hyperref} % This lets you make links to web locations
\graphicspath{ {./images/} }

\usepackage[rightcaption]{sidecap}
\usepackage{subcaption}
\usepackage{wrapfig}

\usepackage{float}

\usepackage{imakeidx}

\usepackage{listings}

\usepackage{pgf-umlsd}

\usepackage{amssymb}

\makeindex


\title{SIM2REAL}
\author{Olivier Sutter \and Mathieu Verm \and Michael Romagne \and Alexandre Martin \and Pablo Mirales \and Vincent Coyette}

\date{\today}

\begin{document}
\maketitle{}

\tableofcontents

\clearpage
\newpage

\section{Introduction}
Reinforcement Rearning is an area of machine learning. The goal is for an agent to learn an optimal behaviour and adapt its comportement, based on experience, in a given environment . Reinforcement Learning has been very successful in the last few years, notably for its great performance in games (e.g. AlphaGo\footnote{Silver, D., Schrittwieser, J., Simonyan, K. et al. Mastering the game of Go without human knowledge. Nature 550, 354â€“359 (2017)}). However, it suffers from a lack of industrial applications.

One of the reason for this is that an agent needs a lot of experience in order to learn a reasonable behaviour. Those simulations must be run in a simulator for safety, speed and financial reasons. Moreover, the transfer of behaviour from the simulated agent to the real world (for example an autonomous vehicle) is far from being trivial. Even for high quality simulators, there is a shift in both states and transitions spaces between the simulator and the real world.

The purpose of this project is to study different methods to robustly train an autonomous car in a simulator, so that the real-world agent behaves as suited.


DuckieTown\footnote{See https://www.duckietown.org/} environment will be used during all the project. It provides a simulator (Gym-Duckietown), a real-world agent and a real-world environment. 


\section{Environment}
An environment in which a Reinforcement Learning agent is trained is represented by a Markov Decision Process (MDP). A MDP is basically a 4-tuple $(S, A, P_a, R_a)$. Let's detail each of these components for duckietown. 

\subsection{State}
In a MDP, the state is defined by the state of the environment as it is seen by the agent. Here, the robot contains only one sensor, the camera. The camera has a resolution of 160x120, and thus the state space is $S = [0, 255]^{160x120x3}$ (3 is for the 3 RGB colors).

\subsection{Actions}
The actions of the environment can take are composed by a forward velocity and a steering angle. By default, the action space is continuous, $A = [-1, 1]^2$:
\begin{itemize}
    \item The first number corresponds to the forward velocity. 1 is for going full speed forward, -1 is for full speed backward.
    \item The second number corresponds to the steering angle. 1 is for the steering wheel being fully pushed to the left.
\end{itemize}

The action space can be changed to be discrete. In this case, the possible actions would be moving forward, turning left or turning right.

\subsection{Transitions}
The transition of a MDP is the distribution $P(s'| s, a)$, i.e. the probability of ending up in the state $s'$ if the agent is in the state s and take the action a. The transitions would typically be different inside the simulator and in real life.

Inside the simulator, the transitions are considered as perfect. For an agent being at a given position and taking a certain action  ${speed, angle}$, the simulator updates the position of the agent accordingly. The state $s'$ is then the new camera signal. The state is not fully deterministic, as some objects (such as duckie pedestrians for example) may have a stochastic behaviour and appear on the camera, but the updated position of the agent is a deterministic function of the current
position and the current action.

In the real world however, the transitions between the positions are not perfect. Given a position and an action, the next position is not deterministic. It can slightly vary because of mechanical links not being perfect, because of unusual road surface. This shift in the transition distribution is one of the problem we will have to address during the simulation to reality transfer.

\subsection{Reward}
The choice of the reward function is primordial to properly train the agent. The reward may depend on :

\begin{itemize}
    \item The position of the agent. The position may be used to compute the distance from a target position, and the distance from the middle of the lane.
    \item The speed of the agent. The faster the better.
    \item Collisions. The agent must get a negative reward in case of collision, and possibly if it gets too  close to another object.
\end{itemize}

The design of the reward function will condition the performance of our agent in the simulator. This reward will be used to train a policy, i.e. a mapping from a state $s$ to an action $a$ to take. This policy will need to be adapted before transfer to the real world. However, the reward does not need to be adapted. 

\section{Gym-DuckieTown Simulator}

From the README.md in the github repository\footnote{https://github.com/duckietown/gym-duckietown}, here is an introduction to Gym-Duckietown. 

 \begin{quotation}
Gym-Duckietown is a simulator for the Duckietown Universe, written in pure Python/OpenGL (Pyglet). It puts the agent, a Duckiebot, inside of an instance of a Duckietown: a loop of roads with turns, intersections, obstacles, Duckie pedestrians, and other Duckiebots. It can be a pretty hectic place!
 \end{quotation}

 This project has a dedicated github repository\footnote{https://github.com/vcoyette/gym-duckietown}, which is a fork from the Gym-Duckietown original repo.

\subsection{Installation}
The installation is pretty straight-forward from the repository. Use the following commands :

\begin{lstlisting}[language=bash]
    git clone https://github.com/vcoyette/gym-duckietown
    cd gym-duckietown
    conda env create -f environment.yaml
\end{lstlisting}

The installation has been tested on Windows, Linux and MacOS. Some problems may be encountered for the installation of certain packages. They can be resolved with package-specific installation instructions. 
For example, the installation of pyglet may raise an issue. It can be resolved by installing it from the pyglet github repository. 

To use the simulator, the environment must be activated :
 
\begin{lstlisting}[language=bash]
    conda activate gym-duckietown
\end{lstlisting}

And the root folder of the project must be added to the PYTHONPATH environment variable.
On linux :
\begin{lstlisting}[language=bash]
    export PYTHONPATH="${PYTHONPATH}:`pwd`"
\end{lstlisting}

On Windows, environment variable can be accessed in the parameters,  advanced parameters. You can then append the path of your project folder to the PYTHONPATH variable if it exists, or create it otherwise.


\subsection{Manual Control}
A UI application can be launched to manually control the robot. Actions can be sent from the keyboard, and images from the DuckieBot camera are displayed. Here is a simple command to launch the application :

\begin{lstlisting}{laguage=bash}
$ ./manual_control.py --env-name Duckietown-udem1-v0
\end{lstlisting}

\subsubsection{Parameters}
Here is a list of parameters which can be used. 

\begin{itemize}
	\item env-name: the name of the environment to execute (TODO anchor to env description)
	\item map-name: the name of the map to be used
	\item distortion: boolean, add distorsion
	\item draw-curve: boolean, draw the lane-following curve
	\item draw-bbox: boolean, draw the collision bounding box
	\item domain-rand: boolean, use domain randomization
	\item frame-skip: number of frames to skip (default 1)
	\item seed: seed
\end{itemize}

\subsubsection{Keyboard}
Here is a list of keys which can be used during the simulation.

\begin{itemize}
	\item Escape : exit simulation
	\item Backspace : restart simulation
	\item Directional keys: go forward, backward or turn
	\item Shift : boost speed
\end{itemize}

\subsubsection{Logs}
The github repository contains a branch "experiment". This branch is to be used to do any experiment on the simulator. The main difference from a classical "develop" branch is that it does not aim at being merged into the master branch. 

In this branch, we designed a logger for the manual control application. At each time step, this logger will log the current position of the agent, the current speed, the current distance from the lane, the action took. The goal is to manually control the agent so that it behaves as we would expect. We would then be able to check the logs, and use them to design a reward function.

To enable this option, pass the --output option to manual\_control.py. You can optionally specify another option `--filename example.csv` to specify the name of the output file, which would be in data/example.csv. If no file-name is specified, the logs will be stored in data/manual\_controli.csv, where i is the first number for which this path is free.

When the episode is done, the manual control must be exited by pressing the S key to save the output.

The learning is performed using the train\_reinforcement.py script. In order to execute it, change into the directory learning and run : 

\begin{lstlisting}{laguage=bash}
    python -m reinforcement.pytorch.train_reinforcement
\end{lstlisting}

If an error is returned stating that a module doesn't exist, check if PYTHONPATH variable contains the repository base folder.

\begin{lstlisting}{laguage=bash}
    echo $PYTHONPATH
\end{lstlisting}

If it doesn't, get back to your to the root directory of the project and run: 

\begin{lstlisting}{laguage=bash}
    export PYTHONPATH="${PYTHONPATH}:`pwd`"
\end{lstlisting}

The default algorithm implemented is DDPG. It can be tweaked by passing some hyperparameters when executing train\_reinforcement. A list of the parameters can be accessed by running :

\begin{lstlisting}{language=bash}
    python -m reinforcement.pytorch.train_reinforcement --h
\end{lstlisting}

\subsection{Create Maps}
You can very easily create a new \textbf{Duckietown} environment with a text editor. A Duckietown's map is a \textit{.yaml} file, so you have to save your new map in the folder \textbf{maps} as "\textit{my\_new\_map.yaml}" (the path should looks like this one : \textit{./gym-duckietown-master/gym\_duckietown/maps}).
\newline

If you want to see your map, you can use the following line in the terminal : \newline

\begin{lstlisting}{language=bash}
./manual_control.py --env-name Duckietown-udem1-v0 
--map-name my\_new\_map}
\end{lstlisting}

\noindent Your robot will be manually controlled in your map.\newline


\noindent A grassroots level of your \textit{.yaml} should looks like this :\newline
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

\noindent tiles:\newline

\noindent- [floor, floor, floor        , grass        , grass        , grass, floor, floor]\newline
\noindent- [floor, floor       , grass        , grass        , straight/S   , grass, floor, floor]\newline
\noindent- [floor, grass       , grass        , curve\_left/W , curve\_right/S, grass, floor, floor]\newline
\noindent- [grass, grass       , curve\_left/W , curve\_right/S, grass        , grass, floor, floor]\newline
\noindent- [grass, curve\_left/W, curve\_right/S, grass        , grass        , floor, floor, floor]\newline
\noindent- [grass, curve\_left/S, curve\_right/E, grass        , grass        , floor, floor, floor]\newline
\noindent- [grass, grass       , curve\_left/S , curve\_right/E, grass        , floor, floor, floor]\newline
\noindent- [floor, grass       , curve\_left/W , curve\_right/S, grass        , floor, floor, floor]\newline
\noindent- [floor, grass       , straight/S   , grass        , grass        , floor, floor, floor]\newline
\noindent- [floor, grass       , grass        , grass        , floor        , floor, floor, floor]\newline
\noindent- [floor, floor       , floor        , floor        , floor        , floor, floor, floor]\newline

\noindent objects:\newline

\noindent- kind: house\newline
  pos: [4.5, 9.1]\newline
  rotate: 90\newline
  height: 0.5\newline

\noindent- kind: tree\newline
  pos: [1, 1]\newline
  rotate: 0\newline
  height: 0.5\newline

\noindent- kind: tree\newline
  pos: [2, 8.5]\newline
  rotate: 90\newline
  height: 0.3\newline

\noindent tile\_size: 0.585\newline
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\newline

For each line, the number of tiles has to remain the same. The \textbf{tile\_size} and the \textbf{height} of every object can change, but insofar as your goal is to transpose the simulation to the real world, you must not change their values. If so, you should have \textbf{tile\_size}$=0.585$ (for the conventional heights of the objects will be given below).\newline

You can find your way in the map knowing that going upward is going North, and knowing that when you drive on a road tile, the name of the road tile tells you which direction you were facing when you arrived on it, and by which direction you will leave the tile. Let's give an example : the tile "\textit{curve\_left/W}" means that you arrived on it while you were moving West, and the fact that you'll turn left on it says you are going to leave the tile by the South (it's of course reversible, you can arrive from South facing North, and leave to the East).\newline
You can also know your position $(x,\ y)\in\mathbb{R}^2$ on the map (this is the way you put objects on it). The point $[0,\ 0]$ matches the upper left corner of the upper left tile of the map (so the coordinates start at the very North/West). The coordinates keep going higher as you move toward South/East, increasing of 1 each time you go through one tile.\newline

Here is the list of the tiles you can use to build your map:

\noindent- straight\newline
\noindent- curve\_left\newline
\noindent- curve\_right\newline
\noindent- 3way\_left (3-way intersection)\newline
\noindent- 3way\_right\newline
\noindent- 4way (4-way intersection)\newline
\noindent- asphalt\newline
\noindent- grass\newline
\noindent- floor (office floor)\newline

You can (and should) orientate the roads (the six first tiles on the list) by adding "/N", "/E", "/S", "/W" (this will be oriented according to the rule given above).\newline

The objects can be added as shown in the following example (changing the name of the object). Here is a list of them :\newline

\noindent- barrier (height : 0.08)\newline
\noindent- cone (height : 0.08)\newline
\noindent- duckie (height : as you wish between 0.06 and 0.08)\newline
\noindent- duckiebot (height : 0.12)\newline
\noindent- tree (height : as you wish between 0.1 and 0.9)\newline
\noindent- house (height : 0.5)\newline
\noindent- truck (height : 0.25)\newline
\noindent- bus (height : 0.18)\newline
\noindent- building (height : 0.6)\newline
\noindent- sign\_stop, sign\_T\_intersect, sign\_yield, etc... (height : $0.18$ for the signs, $0.4$ for a traffic light)\newline

There are many other signs, you can check the whole list here :\newline
https://github.com/duckietown/gym-duckietown/blob/master/gym\_duckietown/meshes\newline

It is possible to add the attributes : \newline

\noindent- optional: True or False (makes the object optional)\newline
\noindent- static: True or False (for the Duckiebot for example if you want to see them move)\newline

To go any further about map creation, check the Github of Duckietown on this link : https://github.com/vcoyette/gym-duckietown/tree/documentation#design\newline

\subsection{Reward Wrapper}
The reward function is computed by the compute\_reward function defined in the class Simulator (in the file gym\_duckietown/simulator.py). The best way to modify the reward function is to edit the function reward of the DtRewardWrapper class in the file learning/utils/wrappers.py.

This class is a subclass of the gym RewardWrapper class. This class implements the Wrapper/Adapter design pattern. It overrides the step function of the simulator. It is hard to explain, but you can have a look at the following sequence diagram to get an idea of how it works.

The idea is that the reward wrapper replaces the environment in the DDPG implementation. During the training, the DDPG calls the step function of the wrapper. This wrapper instance has an attribute which is the environment. The step function in the wrapper calls the function step of the environment, and captures the returned values (observation, reward, done and info). Then, the wrapper runs its own reward function with the reward returned by the previous call as parameter. The wrapper step function returns the values of observation, done and info which were returned by the environment, and returns its custom reward to replace the reward of the environment.

The wrapper containing an environment attribute, it has access to the same information as the simulator to compute its custom reward. For example, it can use the current position through env.cur\_pos, the angle through env.angle, etc.


\begin{sequencediagram}
    \newthread{t}{DDPG}
    \newinst[1]{w}{DtRewardWrapper}
    \newinst[2]{s}{Simulator}
    \begin{call}{t}{step()}{w}{\shortstack{
        return obs, \\ reward(reward) \\
        done, \\ info }}
    

        \begin{call}{w}{step()}{s}{\shortstack{
            return obs, \\ reward, \\
            done, \\ info }}
            \postlevel
            \postlevel
            \postlevel
        \end{call}
    \postlevel
    \postlevel
    \postlevel
    \end{call}
\end{sequencediagram}

TODO : add legend to diagram






\end{document}


